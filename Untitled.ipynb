{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introspector workbook\n",
    "\n",
    "multiple attribute evaluation\n",
    "\n",
    "for each attribute (sorted by occurance):\n",
    " for each other attribute(sorted by occurance) :\n",
    "   what is the probability that this attribute  will co-occur with that attribute?\n",
    "   what is the probability that this attribute value will determine that attribute value?\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a list of attributes\n",
    "import json\n",
    "import pprint\n",
    "from collections import Counter\n",
    "from itertools import combinations,permutations\n",
    "\n",
    "identifier_field = \"_id\"\n",
    "type_field = \"_type\"\n",
    "\n",
    "known= [identifier_field,type_field]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import lzma\n",
    "except ImportError:\n",
    "    from backports import lzma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = Counter()\n",
    "pairs = Counter()\n",
    "\n",
    "with  lzma.open(\"expanded_data.json.xz\") as fi:\n",
    "    for li in fi:\n",
    "        row = json.loads(li)\n",
    "        type_name = row[type_field]\n",
    "        _id = row[identifier_field]\n",
    "        del row[identifier_field]\n",
    "        del row[type_field]\n",
    "        for f in ('srcp','chain','_string_len',\n",
    "                  'scpe',\n",
    "                  '_level', 'type__id'\n",
    "        ):\n",
    "            if f in row:\n",
    "                del row[f]\n",
    "\n",
    "        for f in dict(row):\n",
    "            for f2 in ('srcp','chain','_string_len',\n",
    "                       'scpe','_type',\n",
    "                      '_level', '_id'\n",
    "            ):\n",
    "                if f2 in f:\n",
    "                    if f in row:\n",
    "                        del row[f]\n",
    "\n",
    "        field_list = list(sorted(row.keys()))\n",
    "        types[str(field_list)] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for p in types.most_common(100):\n",
    "    field_list = eval(p[0])\n",
    "    count = p[1]\n",
    "    for p in combinations(field_list, 2):\n",
    "        pairs[str(p)] += count\n",
    "    for p in combinations(field_list, 3):\n",
    "        pairs[str(p)] += count\n",
    "    for p in combinations(field_list, 4):\n",
    "        pairs[str(p)] += count\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"('type_algn', 'type_size')\", 12169)\n",
      "(\"('body', 'link')\", 10970)\n",
      "(\"('name', 'type')\", 10275)\n",
      "(\"('OP0 :', 'type')\", 9652)\n",
      "(\"('name__string', 'type_algn')\", 7849)\n",
      "(\"('name__string', 'type_size')\", 7849)\n",
      "(\"('name__string', 'type_algn', 'type_size')\", 7849)\n",
      "(\"('algn', 'size')\", 7712)\n",
      "(\"('link', 'name__string')\", 7208)\n",
      "(\"('body', 'name__string')\", 6903)\n"
     ]
    }
   ],
   "source": [
    "for p in pairs.most_common(10):\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-9e7137db6385>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mname__strings\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'name__string'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "name__strings = list(df['name__string'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we load the word vector\n",
    "# naming convention extractor\n",
    "import re\n",
    "import collections\n",
    "\n",
    "splitters = [\n",
    "    '_____',\n",
    "    '____',\n",
    "    '___',\n",
    "    '__',\n",
    "    '_',\n",
    "    ' ',\n",
    "    '*',\n",
    "    '.',\n",
    "    \"/\"\n",
    "]\n",
    "def camel_to_snake(name):\n",
    "  name = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "  return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', name)\n",
    "\n",
    "# camelcase\n",
    "chars = collections.Counter()\n",
    "\n",
    "for l in name__strings:\n",
    "    l = l.strip().strip(\"\\\"\")\n",
    "    parts = { camel_to_snake(l) : 1}\n",
    "   \n",
    "    for s in splitters:\n",
    "        for p in dict(parts):\n",
    "            for s2 in p.split(s):\n",
    "                if s2 not in parts:\n",
    "                    parts[s2] =1\n",
    "                    if p in parts:\n",
    "                        del[parts[p]]\n",
    "                    #print(p, s2)\n",
    "\n",
    "    for c in parts:\n",
    "        chars[c]+=1\n",
    "del chars['']\n",
    "\n",
    "chars.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "name__strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [f for f in list(chars.keys()) if len(f) > 2]\n",
    "names.append(\"_\")\n",
    "names.append(\"__\")\n",
    "names.append(\"___\")\n",
    "names.append(\"____\")\n",
    "matcha_re = re.compile(r\"(?=(\"+'|'.join(names)+r\"))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "allnames = []\n",
    "allnames.extend(name__strings)\n",
    "allnames.extend(names)\n",
    "\n",
    "allnames_env = OneHotEncoder(drop='first').fit([ [f,1] for f in allnames])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "allnames_env.categories_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# https://datatofish.com/create-pandas-dataframe/\n",
    "names = ('name__string', 'type_algn')    \n",
    "data= {}\n",
    "for n in names:\n",
    "    data[n] = []\n",
    "\n",
    "with lzma.open(\"expanded_data.json.xz\") as fi:\n",
    "    for li in fi:\n",
    "        row = json.loads(li)\n",
    "            \n",
    "        exists = [f in row for f in names]\n",
    "        if all(exists):\n",
    "            #vals = [row[f] for f in names]\n",
    "            #print(vals)\n",
    "            for f in names:\n",
    "                data[f].append(row[f])\n",
    "df = pd.DataFrame (data, columns = names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://stackoverflow.com/questions/58101126/using-scikit-learn-onehotencoder-with-a-pandas-dataframe\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "jobs_encoder = LabelBinarizer()\n",
    "jobs_encoder.fit(allnames)\n",
    "transformed = jobs_encoder.transform(df['name__string'])\n",
    "ohe_df = pd.DataFrame(transformed)\n",
    "df = pd.concat([df, ohe_df], axis=1)\n",
    "#.drop(['name__string'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for each word, create a vector \n",
    "substring_matches = [ [m for m in matcha_re.findall(n)] for n in df['name__string']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how many matches do we have\n",
    "max([ len(x) for x in substring_matches])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substring_matches_df = pd.DataFrame(data=substring_matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "substring_matches_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_all = [jobs_encoder.transform(substring_matches_df[c].fillna(\"novaluehere\")) for c in substring_matches_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed_all_data = [pd.DataFrame(transformed_all[c]) for c in substring_matches_df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = transformed_all_data[0]\n",
    "for c in substring_matches_df.columns:\n",
    "    if c > 0:\n",
    "        temp = temp + transformed_all_data[c]\n",
    "        \n",
    "temp = temp + ohe_df\n",
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ohe_df = temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "X = ohe_df\n",
    "y = df['type_algn']\n",
    "#X_train, X_test, y_train, y_test = train_test_split(\n",
    "#    X, y, random_state=42\n",
    "#)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert y into one hot encoding as well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_encoder = LabelBinarizer()\n",
    "y_encoder.fit(y)\n",
    "y_transformed = y_encoder.transform(y)\n",
    "y_df = pd.DataFrame(y_transformed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(y_encoder.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_weight = np.random.RandomState(42).rand(y.shape[0])\n",
    "X_train, X_test, y_train, y_test, sw_train, sw_test = \\\n",
    "    train_test_split(X, y_df, sample_weight, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(X_train.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(X_test.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.compose import TransformedTargetRegressor\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "#model = TransformedTargetRegressor(        regressor=Ridge(alpha=1e-10),func=np.log10,        inverse_func=sp.special.exp10)\n",
    "#model = GaussianNB()\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "model = MLPClassifier(solver='lbfgs', alpha=1e-5,\n",
    "                      verbose=10,\n",
    "                    hidden_layer_sizes=(150, 15, 4, 15), random_state=1)\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/calibration/plot_calibration.html#sphx-glr-auto-examples-calibration-plot-calibration-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training set score: %f\" % model.score(X_train, y_train))\n",
    "print(\"Test set score: %f\" % model.score(X_test, y_test))\n",
    "\n",
    "#fig, axes = plt.subplots(4, 4)\n",
    "# use global min / max to ensure all weights are shown on the same scale\n",
    "vmin, vmax = model.coefs_[0].min(), model.coefs_[0].max()\n",
    "for coef in model.coefs_:\n",
    "    print(len(coef))\n",
    "    plt.plot(coef)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/inspection/plot_linear_model_coefficient_interpretation.html#sphx-glr-auto-examples-inspection-plot-linear-model-coefficient-interpretation-py\n",
    "from sklearn.metrics import median_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "y_pred = model.predict(X_train)\n",
    "\n",
    "mae = median_absolute_error(y_train, y_pred)\n",
    "string_score = f'MAE on training set: {mae:.2f} '\n",
    "y_pred = model.predict(X_test)\n",
    "mae = median_absolute_error(y_test, y_pred)\n",
    "string_score += f'\\nMAE on testing set: {mae:.2f}'\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "plt.scatter(y_test, y_pred)\n",
    "ax.plot([0, 1], [0, 1], transform=ax.transAxes, ls=\"--\", c=\"red\")\n",
    "plt.text(3, 20, string_score)\n",
    "plt.title('Ridge model, small regularization')\n",
    "plt.ylabel('Model predictions')\n",
    "plt.xlabel('Truths')\n",
    "\n",
    "_ = plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from https://towardsdatascience.com/quickly-test-multiple-models-a98477476f0\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn import model_selection\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from matplotlib.colors import ListedColormap\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import make_moons, make_circles, make_classification\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.gaussian_process import GaussianProcessClassifier\n",
    "from sklearn.gaussian_process.kernels import RBF\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "current_milli_time = lambda: int(round(time.time() * 1000))\n",
    "\n",
    "def run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n",
    "    '''\n",
    "    Lightweight script to test many models and find winners\n",
    ":param X_train: training split\n",
    "    :param y_train: training target vector\n",
    "    :param X_test: test split\n",
    "    :param y_test: test target vector\n",
    "    :return: DataFrame of predictions\n",
    "    '''\n",
    "    \n",
    "    dfs = []\n",
    "    models = [\n",
    "        #(\"GaussianNB\",GaussianNB()),\n",
    "        \n",
    "        #('LogReg', LogisticRegression()), \n",
    "          ('RF', RandomForestClassifier()),\n",
    "          ('KNN', KNeighborsClassifier()),\n",
    "          ('SVM', SVC()), \n",
    "          ('GNB', GaussianNB()),\n",
    "          ('XGB', XGBClassifier()),\n",
    "          ('KNeighborsClassifier',KNeighborsClassifier(3)),\n",
    "          ('svc1',SVC(kernel=\"linear\", C=0.025)),\n",
    "          ('svc2',SVC(gamma=2, C=1)),\n",
    "          ('GaussianProcessClassifier',GaussianProcessClassifier(1.0 * RBF(1.0))),\n",
    "          ('DecisionTreeClassifier',DecisionTreeClassifier(max_depth=5)),\n",
    "          ('RandomForestClassifier',RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)),\n",
    "          ('MLPClassifier',MLPClassifier(alpha=1, max_iter=1000)),\n",
    "          ('AdaBoostClassifier',AdaBoostClassifier()),\n",
    "          ('QuadraticDiscriminantAnalysis',QuadraticDiscriminantAnalysis())\n",
    "    ]\n",
    "    results = []\n",
    "    names = []\n",
    "    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n",
    "    \n",
    "    for name, model in models:\n",
    "            print(name)\n",
    "            start_time = current_milli_time()\n",
    "            kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n",
    "            cv_results = model_selection.cross_validate(model, X_train, y_train\n",
    "                                                        #, cv=kfold\n",
    "                                                        #, scoring=scoring\n",
    "                                                       )\n",
    "            try :\n",
    "                clf = model.fit(X_train, y_train)\n",
    "                y_pred = clf.predict(X_test)\n",
    "            \n",
    "                print(classification_report(y_test, y_pred\n",
    "                                            #, target_names=target_names\n",
    "                                       )\n",
    "                                       )\n",
    "                results.append(cv_results)\n",
    "                names.append(name)\n",
    "            except Exception as e:\n",
    "                print (e)\n",
    "            end_time = current_milli_time()\n",
    "            print(end_time - start_time)\n",
    "            this_df = pd.DataFrame(cv_results)\n",
    "            this_df['model'] = name\n",
    "            dfs.append(this_df)\n",
    "    final = pd.concat(dfs, ignore_index=True)\n",
    "    return final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_exps(X_train, y_train, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(X.mean()) # plotting by columns\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Sequential()\n",
    "\n",
    "n_cols = X_train.shape[1]\n",
    "o_cols = y_train.shape[1]\n",
    "model2.add(Dense(500, activation='relu',input_shape=(n_cols,)))\n",
    "model2.add(Dense(100), activation='relu')\n",
    "model2.add(Dense(o_cols), activation='softmax')\n",
    "\n",
    "model2.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " model2.fit(X_train, y_train, verbose=10, batch_size=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2.evaluate(X_test, y_test, verbose=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model2.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pydot\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "plot_model(model2, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "! pip install pydot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "reload(keras.utils.vis_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.utils.vis_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
